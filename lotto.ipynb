{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sandg\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'h5py==2.10.0'\": Expected package name at the start of dependency specifier\n",
      "    'h5py==2.10.0'\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "# !pip install tensorflow-gpu==2.0.0-rc1\n",
    "!pip install 'h5py==2.10.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rows = np.loadtxt(\"./lotto.csv\", delimiter=\",\")\n",
    "row_count = len(rows)\n",
    "print(row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def numbers2ohbin(numbers):\n",
    "\n",
    "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
    "\n",
    "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
    "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
    "    \n",
    "    return ohbin\n",
    "\n",
    "#원핫인코딩으로 변환\n",
    "def ohbin2numbers(ohbin):\n",
    "\n",
    "    numbers = []\n",
    "    \n",
    "    for i in range(len(ohbin)):\n",
    "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
    "            numbers.append(i+1)\n",
    "    \n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ohbins\n",
      "X[0]: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Y[0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "numbers\n",
      "X[0]: [2, 16, 19, 31, 34, 35]\n",
      "Y[0]: [13, 14, 22, 27, 30, 38]\n"
     ]
    }
   ],
   "source": [
    "numbers = rows[:, 1:7]\n",
    "ohbins = list(map(numbers2ohbin, numbers))\n",
    "\n",
    "x_samples = ohbins[0:row_count-1]\n",
    "y_samples = ohbins[1:row_count]\n",
    "\n",
    "#원핫인코딩으로 표시\n",
    "print(\"ohbins\")\n",
    "print(\"X[0]: \" + str(x_samples[0]))\n",
    "print(\"Y[0]: \" + str(y_samples[0]))\n",
    "\n",
    "#번호로 표시\n",
    "print(\"numbers\")\n",
    "print(\"X[0]: \" + str(ohbin2numbers(x_samples[0])))\n",
    "print(\"Y[0]: \" + str(ohbin2numbers(y_samples[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = (0, 400) #훈련\n",
    "val_idx = (401, 500) #검증\n",
    "test_idx = (501, len(x_samples)) #시험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# 모델을 정의합니다.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
    "    keras.layers.Dense(45, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    0 train acc 0.025 loss 0.418 val acc 0.040 loss 0.399\n",
      "epoch    1 train acc 0.037 loss 0.398 val acc 0.040 loss 0.398\n",
      "epoch    2 train acc 0.043 loss 0.394 val acc 0.040 loss 0.398\n",
      "epoch    3 train acc 0.037 loss 0.392 val acc 0.040 loss 0.398\n",
      "epoch    4 train acc 0.037 loss 0.388 val acc 0.030 loss 0.399\n",
      "epoch    5 train acc 0.035 loss 0.383 val acc 0.010 loss 0.401\n",
      "epoch    6 train acc 0.045 loss 0.376 val acc 0.020 loss 0.404\n",
      "epoch    7 train acc 0.043 loss 0.369 val acc 0.020 loss 0.408\n",
      "epoch    8 train acc 0.048 loss 0.360 val acc 0.020 loss 0.413\n",
      "epoch    9 train acc 0.060 loss 0.352 val acc 0.020 loss 0.418\n",
      "epoch   10 train acc 0.068 loss 0.344 val acc 0.010 loss 0.423\n",
      "epoch   11 train acc 0.072 loss 0.335 val acc 0.010 loss 0.427\n",
      "epoch   12 train acc 0.090 loss 0.327 val acc 0.020 loss 0.432\n",
      "epoch   13 train acc 0.087 loss 0.318 val acc 0.020 loss 0.436\n",
      "epoch   14 train acc 0.095 loss 0.309 val acc 0.010 loss 0.442\n",
      "epoch   15 train acc 0.115 loss 0.299 val acc 0.010 loss 0.448\n",
      "epoch   16 train acc 0.110 loss 0.289 val acc 0.020 loss 0.455\n",
      "epoch   17 train acc 0.117 loss 0.279 val acc 0.020 loss 0.461\n",
      "epoch   18 train acc 0.117 loss 0.269 val acc 0.020 loss 0.468\n",
      "epoch   19 train acc 0.115 loss 0.259 val acc 0.020 loss 0.475\n",
      "epoch   20 train acc 0.105 loss 0.248 val acc 0.020 loss 0.484\n",
      "epoch   21 train acc 0.092 loss 0.237 val acc 0.020 loss 0.492\n",
      "epoch   22 train acc 0.098 loss 0.226 val acc 0.020 loss 0.501\n",
      "epoch   23 train acc 0.100 loss 0.215 val acc 0.030 loss 0.510\n",
      "epoch   24 train acc 0.090 loss 0.203 val acc 0.030 loss 0.520\n",
      "epoch   25 train acc 0.095 loss 0.191 val acc 0.020 loss 0.530\n",
      "epoch   26 train acc 0.113 loss 0.180 val acc 0.030 loss 0.541\n",
      "epoch   27 train acc 0.122 loss 0.169 val acc 0.020 loss 0.551\n",
      "epoch   28 train acc 0.120 loss 0.158 val acc 0.020 loss 0.562\n",
      "epoch   29 train acc 0.160 loss 0.147 val acc 0.010 loss 0.575\n",
      "epoch   30 train acc 0.150 loss 0.137 val acc 0.010 loss 0.591\n",
      "epoch   31 train acc 0.177 loss 0.127 val acc 0.010 loss 0.604\n",
      "epoch   32 train acc 0.177 loss 0.118 val acc 0.010 loss 0.617\n",
      "epoch   33 train acc 0.150 loss 0.110 val acc 0.010 loss 0.628\n",
      "epoch   34 train acc 0.140 loss 0.103 val acc 0.010 loss 0.641\n",
      "epoch   35 train acc 0.152 loss 0.097 val acc 0.010 loss 0.652\n",
      "epoch   36 train acc 0.158 loss 0.089 val acc 0.020 loss 0.668\n",
      "epoch   37 train acc 0.142 loss 0.086 val acc 0.010 loss 0.676\n",
      "epoch   38 train acc 0.158 loss 0.077 val acc 0.020 loss 0.688\n",
      "epoch   39 train acc 0.177 loss 0.073 val acc 0.030 loss 0.714\n",
      "epoch   40 train acc 0.140 loss 0.068 val acc 0.030 loss 0.719\n",
      "epoch   41 train acc 0.163 loss 0.064 val acc 0.051 loss 0.724\n",
      "epoch   42 train acc 0.150 loss 0.060 val acc 0.040 loss 0.740\n",
      "epoch   43 train acc 0.138 loss 0.054 val acc 0.030 loss 0.752\n",
      "epoch   44 train acc 0.160 loss 0.052 val acc 0.030 loss 0.757\n",
      "epoch   45 train acc 0.155 loss 0.046 val acc 0.010 loss 0.782\n",
      "epoch   46 train acc 0.190 loss 0.047 val acc 0.010 loss 0.791\n",
      "epoch   47 train acc 0.145 loss 0.042 val acc 0.020 loss 0.810\n",
      "epoch   48 train acc 0.150 loss 0.039 val acc 0.010 loss 0.812\n",
      "epoch   49 train acc 0.163 loss 0.034 val acc 0.000 loss 0.827\n",
      "epoch   50 train acc 0.147 loss 0.036 val acc 0.010 loss 0.834\n",
      "epoch   51 train acc 0.168 loss 0.032 val acc 0.030 loss 0.845\n",
      "epoch   52 train acc 0.177 loss 0.031 val acc 0.020 loss 0.858\n",
      "epoch   53 train acc 0.168 loss 0.027 val acc 0.010 loss 0.866\n",
      "epoch   54 train acc 0.175 loss 0.026 val acc 0.030 loss 0.889\n",
      "epoch   55 train acc 0.163 loss 0.024 val acc 0.010 loss 0.923\n",
      "epoch   56 train acc 0.147 loss 0.022 val acc 0.040 loss 0.918\n",
      "epoch   57 train acc 0.160 loss 0.024 val acc 0.020 loss 0.922\n",
      "epoch   58 train acc 0.160 loss 0.023 val acc 0.030 loss 0.929\n",
      "epoch   59 train acc 0.150 loss 0.020 val acc 0.010 loss 0.951\n",
      "epoch   60 train acc 0.190 loss 0.017 val acc 0.010 loss 0.966\n",
      "epoch   61 train acc 0.165 loss 0.016 val acc 0.010 loss 0.982\n",
      "epoch   62 train acc 0.152 loss 0.014 val acc 0.000 loss 1.003\n",
      "epoch   63 train acc 0.147 loss 0.017 val acc 0.010 loss 0.975\n",
      "epoch   64 train acc 0.155 loss 0.015 val acc 0.010 loss 1.021\n",
      "epoch   65 train acc 0.168 loss 0.012 val acc 0.000 loss 1.009\n",
      "epoch   66 train acc 0.170 loss 0.010 val acc 0.010 loss 1.029\n",
      "epoch   67 train acc 0.155 loss 0.010 val acc 0.030 loss 1.040\n",
      "epoch   68 train acc 0.205 loss 0.009 val acc 0.000 loss 1.069\n",
      "epoch   69 train acc 0.160 loss 0.008 val acc 0.010 loss 1.059\n",
      "epoch   70 train acc 0.177 loss 0.013 val acc 0.000 loss 1.085\n",
      "epoch   71 train acc 0.140 loss 0.017 val acc 0.000 loss 1.075\n",
      "epoch   72 train acc 0.120 loss 0.024 val acc 0.040 loss 1.067\n",
      "epoch   73 train acc 0.168 loss 0.013 val acc 0.020 loss 1.092\n",
      "epoch   74 train acc 0.140 loss 0.008 val acc 0.020 loss 1.099\n",
      "epoch   75 train acc 0.160 loss 0.005 val acc 0.020 loss 1.142\n",
      "epoch   76 train acc 0.172 loss 0.004 val acc 0.020 loss 1.152\n",
      "epoch   77 train acc 0.160 loss 0.003 val acc 0.020 loss 1.170\n",
      "epoch   78 train acc 0.155 loss 0.003 val acc 0.010 loss 1.183\n",
      "epoch   79 train acc 0.168 loss 0.003 val acc 0.010 loss 1.184\n",
      "epoch   80 train acc 0.168 loss 0.002 val acc 0.010 loss 1.195\n",
      "epoch   81 train acc 0.165 loss 0.002 val acc 0.010 loss 1.204\n",
      "epoch   82 train acc 0.172 loss 0.002 val acc 0.010 loss 1.227\n",
      "epoch   83 train acc 0.182 loss 0.002 val acc 0.010 loss 1.234\n",
      "epoch   84 train acc 0.175 loss 0.002 val acc 0.010 loss 1.242\n",
      "epoch   85 train acc 0.182 loss 0.002 val acc 0.020 loss 1.262\n",
      "epoch   86 train acc 0.160 loss 0.053 val acc 0.010 loss 1.151\n",
      "epoch   87 train acc 0.207 loss 0.045 val acc 0.020 loss 1.141\n",
      "epoch   88 train acc 0.163 loss 0.013 val acc 0.020 loss 1.173\n",
      "epoch   89 train acc 0.142 loss 0.006 val acc 0.020 loss 1.198\n",
      "epoch   90 train acc 0.150 loss 0.004 val acc 0.020 loss 1.211\n",
      "epoch   91 train acc 0.145 loss 0.003 val acc 0.020 loss 1.229\n",
      "epoch   92 train acc 0.145 loss 0.003 val acc 0.020 loss 1.241\n",
      "epoch   93 train acc 0.140 loss 0.002 val acc 0.020 loss 1.240\n",
      "epoch   94 train acc 0.147 loss 0.002 val acc 0.020 loss 1.253\n",
      "epoch   95 train acc 0.160 loss 0.002 val acc 0.000 loss 1.260\n",
      "epoch   96 train acc 0.165 loss 0.002 val acc 0.000 loss 1.268\n",
      "epoch   97 train acc 0.168 loss 0.001 val acc 0.020 loss 1.280\n",
      "epoch   98 train acc 0.180 loss 0.001 val acc 0.020 loss 1.295\n",
      "epoch   99 train acc 0.172 loss 0.001 val acc 0.020 loss 1.300\n"
     ]
    }
   ],
   "source": [
    "# 매 에포크마다 훈련과 검증의 손실 및 정확도를 기록하기 위한 변수\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "# 최대 100번 에포크까지 수행\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    \n",
    "    for i in range(train_idx[0], train_idx[1]):\n",
    "        \n",
    "        xs = x_samples[i].reshape(1, 1, 45)\n",
    "        ys = y_samples[i].reshape(1, 45)\n",
    "        \n",
    "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
    "\n",
    "        batch_train_loss.append(loss)\n",
    "        batch_train_acc.append(acc)\n",
    "\n",
    "    train_loss.append(np.mean(batch_train_loss))\n",
    "    train_acc.append(np.mean(batch_train_acc))\n",
    "\n",
    "    batch_val_loss = []\n",
    "    batch_val_acc = []\n",
    "\n",
    "    for i in range(val_idx[0], val_idx[1]):\n",
    "\n",
    "        xs = x_samples[i].reshape(1, 1, 45)\n",
    "        ys = y_samples[i].reshape(1, 45)\n",
    "        \n",
    "        loss, acc = model.test_on_batch(xs, ys) #배치만큼 모델에 입력하여 나온 답을 정답과 비교함\n",
    "        \n",
    "        batch_val_loss.append(loss)\n",
    "        batch_val_acc.append(acc)\n",
    "\n",
    "    val_loss.append(np.mean(batch_val_loss))\n",
    "    val_acc.append(np.mean(batch_val_acc))\n",
    "\n",
    "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f} val acc {3:0.3f} loss {4:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss), np.mean(batch_val_acc), np.mean(batch_val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp38-cp38-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp38-cp38-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in c:\\users\\sandg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandg\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading pillow-10.4.0-cp38-cp38-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sandg\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\sandg\\appdata\\roaming\\python\\python38\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.7.5-cp38-cp38-win_amd64.whl (7.5 MB)\n",
      "   ---------------------------------------- 0.0/7.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.5 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.1/7.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.7/7.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.2/7.5 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.0/7.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.5/7.5 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.1.1-cp38-cp38-win_amd64.whl (477 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading kiwisolver-1.4.7-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Downloading pillow-10.4.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.54.1 importlib-resources-6.4.5 kiwisolver-1.4.7 matplotlib-3.7.5 pillow-10.4.0 pyparsing-3.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sandg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sandg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sandg\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m fig, loss_ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      6\u001b[0m acc_ax \u001b[38;5;241m=\u001b[39m loss_ax\u001b[38;5;241m.\u001b[39mtwinx()\n\u001b[1;32m----> 8\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_loss\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m loss_ax\u001b[38;5;241m.\u001b[39mplot(val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m acc_ax\u001b[38;5;241m.\u001b[39mplot(train_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain acc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프로 성능 확인(딥러닝을 학습한다고 해서 검증값은 잘 나오지 않는걸 알 수 있음.)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(train_loss, 'y', label='train loss')\n",
    "loss_ax.plot(val_loss, 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(train_acc, 'b', label='train acc')\n",
    "acc_ax.plot(val_acc, 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2372092029.1044445, 56777152.11777778, 1457124.5777777778, 50000.0, 5000.0]\n"
     ]
    }
   ],
   "source": [
    "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
    "mean_prize = [ np.mean(rows[87:, 8]),\n",
    "           np.mean(rows[87:, 9]),\n",
    "           np.mean(rows[87:, 10]),\n",
    "           np.mean(rows[87:, 11]),\n",
    "           np.mean(rows[87:, 12])]\n",
    "\n",
    "print(mean_prize)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등수와 상금을 반환함\n",
    "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
    "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for ps in pred_numbers:\n",
    "        if ps in true_numbers:\n",
    "            count += 1\n",
    "\n",
    "    if count == 6:\n",
    "        return 0, mean_prize[0]\n",
    "    elif count == 5 and true_bonus in pred_numbers:\n",
    "        return 1, mean_prize[1]\n",
    "    elif count == 5:\n",
    "        return 2, mean_prize[2]\n",
    "    elif count == 4:\n",
    "        return 3, mean_prize[3]\n",
    "    elif count == 3:\n",
    "        return 4, mean_prize[4]\n",
    "\n",
    "    return 5, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_numbers_from_probability(nums_prob):\n",
    "\n",
    "    ball_box = []\n",
    "\n",
    "    for n in range(45):\n",
    "        ball_count = int(nums_prob[n] * 100 + 1)\n",
    "        ball = np.full((ball_count), n+1) #1부터 시작\n",
    "        ball_box += list(ball)\n",
    "\n",
    "    selected_balls = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        if len(selected_balls) == 6:\n",
    "            break\n",
    "        \n",
    "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
    "        ball = ball_box[ball_index]\n",
    "\n",
    "        if ball not in selected_balls:\n",
    "            selected_balls.append(ball)\n",
    "\n",
    "    return selected_balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[No. ] 1st 2nd 3rd 4th 5th 6th Rewards\n",
      "[   1]   4   0   5   1   0   0   9,495,703,739\n",
      "[   2]   5   0   4   1   0   0  11,866,338,643\n",
      "[   3]   5   0   1   4   0   0  11,862,117,270\n",
      "[   4]   1   0   2   7   0   0   2,375,356,278\n",
      "[   5]   3   0   3   4   0   0   7,120,847,461\n",
      "[   6]   3   0   4   3   0   0   7,122,254,585\n",
      "[   7]   4   0   4   2   0   0   9,494,296,614\n",
      "[   8]   1   0   7   2   0   0   2,382,391,901\n",
      "[   9]   3   0   3   3   1   0   7,120,802,461\n",
      "[  10]   5   0   4   1   0   0  11,866,338,643\n",
      "[  11]   4   0   5   1   0   0   9,495,703,739\n",
      "[  12]   2   1   2   4   0   1   4,804,075,459\n",
      "[  13]   4   0   5   1   0   0   9,495,703,739\n",
      "[  14]   0   0   6   3   1   0       8,897,747\n",
      "[  15]   2   2   4   2   0   0   4,863,666,860\n",
      "[  16]   2   0   5   2   1   0   4,751,574,681\n",
      "[  17]   5   0   4   1   0   0  11,866,338,643\n",
      "[  18]   4   1   5   0   0   0   9,552,430,891\n",
      "[  19]   4   0   5   0   1   0   9,495,658,739\n",
      "[  20]   1   0   5   3   1   0   2,379,532,651\n",
      "[  21]   5   0   5   0   0   0  11,867,745,768\n",
      "[  22]   4   0   5   1   0   0   9,495,703,739\n",
      "[  23]   4   0   4   1   1   0   9,494,251,614\n",
      "[  24]   2   0   5   3   0   0   4,751,619,681\n",
      "[  25]   2   0   5   2   1   0   4,751,574,681\n",
      "[  26]   1   2   2   4   1   0   2,488,765,582\n",
      "[  27]   0   0   5   4   1   0       7,490,622\n",
      "[  28]   6   1   3   0   0   0  14,293,700,700\n",
      "[  29]   0   0   5   5   0   0       7,535,622\n",
      "[  30]   0   1   5   3   1   0      64,217,775\n",
      "[  31]   3   1   5   1   0   0   7,180,388,862\n",
      "[  32]   1   0   7   2   0   0   2,382,391,901\n",
      "[  33]   4   1   4   1   0   0   9,551,023,766\n",
      "[  34]   2   0   8   0   0   0   4,755,841,054\n",
      "[  35]   0   0   2   6   2   0       3,224,249\n",
      "[  36]   2   0   4   4   0   0   4,750,212,556\n",
      "[  37]   2   0   4   4   0   0   4,750,212,556\n",
      "[  38]   3   0   7   0   0   0   7,126,475,959\n",
      "[  39]   2   0   6   2   0   0   4,753,026,805\n",
      "[  40]   5   0   4   1   0   0  11,866,338,643\n",
      "[  41]   5   0   5   0   0   0  11,867,745,768\n",
      "[  42]   3   0   6   1   0   0   7,125,068,834\n",
      "[  43]   2   0   6   2   0   0   4,753,026,805\n",
      "[  44]   1   0   2   6   1   0   2,375,311,278\n",
      "[  45]   3   0   6   1   0   0   7,125,068,834\n",
      "[  46]   3   0   6   1   0   0   7,125,068,834\n",
      "[  47]   3   0   6   1   0   0   7,125,068,834\n",
      "[  48]   1   0   6   2   0   1   2,380,934,776\n",
      "[  49]   1   0   5   4   0   0   2,379,577,651\n",
      "[  50]   4   0   3   3   0   0   9,492,889,490\n",
      "[  51]   4   0   2   4   0   0   9,491,482,365\n",
      "[  52]   0   0   4   6   0   0       6,128,498\n",
      "[  53]   3   0   5   2   0   0   7,123,661,710\n",
      "[  54]   4   0   3   2   1   0   9,492,844,490\n",
      "[  55]   0   0   6   3   1   0       8,897,747\n",
      "[  56]   5   0   3   1   1   0  11,864,886,519\n",
      "[  57]   2   0   5   3   0   0   4,751,619,681\n",
      "[  58]   0   0   2   4   3   1       3,129,249\n",
      "[  59]   2   0   2   5   1   0   4,747,353,307\n",
      "[  60]   1   2   3   2   2   0   2,490,127,707\n",
      "[  61]   2   0   7   1   0   0   4,754,433,930\n",
      "[  62]   0   0   4   4   2   0       6,038,498\n",
      "[  63]   1   0   5   2   2   0   2,379,487,651\n",
      "[  64]   1   0   5   4   0   0   2,379,577,651\n",
      "[  65]   2   0   5   2   1   0   4,751,574,681\n",
      "[  66]   1   0   1   7   0   1   2,373,899,153\n",
      "[  67]   1   2   3   4   0   0   2,490,217,707\n",
      "[  68]   1   0   5   3   1   0   2,379,532,651\n",
      "[  69]   1   0   4   4   1   0   2,378,125,527\n",
      "[  70]   0   0   3   4   3   0       4,586,373\n",
      "[  71]   1   0   4   4   1   0   2,378,125,527\n",
      "[  72]   3   0   6   1   0   0   7,125,068,834\n",
      "[  73]   2   0   4   3   1   0   4,750,167,556\n",
      "[  74]   1   0   4   3   2   0   2,378,080,527\n",
      "[  75]   0   0   3   3   4   0       4,541,373\n",
      "[  76]   0   0   6   4   0   0       8,942,747\n",
      "[  77]   1   1   6   0   1   1   2,437,616,928\n",
      "[  78]   0   0   0   7   3   0         365,000\n",
      "[  79]   1   0   6   2   0   1   2,380,934,776\n",
      "[  80]   1   0   5   3   1   0   2,379,532,651\n",
      "[  81]   0   0   0   7   3   0         365,000\n",
      "[  82]   0   0   7   2   1   0      10,304,872\n",
      "[  83]   2   0   4   4   0   0   4,750,212,556\n",
      "[  84]   4   0   5   1   0   0   9,495,703,739\n",
      "[  85]   2   0   6   2   0   0   4,753,026,805\n",
      "[  86]   2   0   5   3   0   0   4,751,619,681\n",
      "[  87]   0   0   4   4   2   0       6,038,498\n",
      "[  88]   3   0   3   4   0   0   7,120,847,461\n",
      "[  89]   0   0   7   3   0   0      10,349,872\n",
      "[  90]   4   0   4   2   0   0   9,494,296,614\n",
      "[  91]   3   0   6   1   0   0   7,125,068,834\n",
      "[  92]   3   1   5   1   0   0   7,180,388,862\n",
      "[  93]   0   0   5   4   1   0       7,490,622\n",
      "[  94]   1   0   1   7   1   0   2,373,904,153\n",
      "[  95]   0   0   5   2   2   1       7,395,622\n",
      "[  96]   0   0   0   7   2   1         360,000\n",
      "[  97]   1   0   5   4   0   0   2,379,577,651\n",
      "[  98]   0   0   4   3   3   0       5,993,498\n",
      "[  99]   0   0   4   3   3   0       5,993,498\n",
      "[ 100]   1   0   2   7   0   0   2,375,356,278\n",
      "[ 101]   0   0   5   4   1   0       7,490,622\n",
      "[ 102]   1   0   7   1   1   0   2,382,346,901\n",
      "[ 103]   0   0   1   5   4   0       1,727,124\n",
      "[ 104]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 105]   1   0   3   4   1   1   2,376,668,402\n",
      "[ 106]   0   0   8   2   0   0      11,756,996\n",
      "[ 107]   0   0   5   3   1   1       7,440,622\n",
      "[ 108]   6   0   3   1   0   0  14,236,973,548\n",
      "[ 109]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 110]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 111]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 112]   3   0   3   4   0   0   7,120,847,461\n",
      "[ 113]   2   0   6   2   0   0   4,753,026,805\n",
      "[ 114]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 115]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 116]   7   0   2   0   1   0  16,607,563,452\n",
      "[ 117]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 118]   4   0   4   1   1   0   9,494,251,614\n",
      "[ 119]   2   1   5   2   0   0   4,808,346,833\n",
      "[ 120]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 121]   4   0   5   0   1   0   9,495,658,739\n",
      "[ 122]   2   2   3   3   0   0   4,862,259,736\n",
      "[ 123]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 124]   1   0   4   3   1   1   2,378,075,527\n",
      "[ 125]   0   0   1   5   3   1       1,722,124\n",
      "[ 126]   0   0   3   5   2   0       4,631,373\n",
      "[ 127]   2   0   5   1   0   2   4,751,519,681\n",
      "[ 128]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 129]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 130]   1   0   5   4   0   0   2,379,577,651\n",
      "[ 131]   1   0   8   1   0   0   2,383,799,025\n",
      "[ 132]   3   0   3   4   0   0   7,120,847,461\n",
      "[ 133]   4   0   6   0   0   0   9,497,110,863\n",
      "[ 134]   2   0   6   2   0   0   4,753,026,805\n",
      "[ 135]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 136]   1   0   5   4   0   0   2,379,577,651\n",
      "[ 137]   2   0   3   5   0   0   4,748,805,431\n",
      "[ 138]   7   0   1   2   0   0  16,606,201,328\n",
      "[ 139]   0   0   5   4   0   1       7,485,622\n",
      "[ 140]   6   0   3   1   0   0  14,236,973,548\n",
      "[ 141]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 142]   0   0   9   1   0   0      13,164,121\n",
      "[ 143]   5   0   2   2   1   0  11,863,479,394\n",
      "[ 144]   4   0   2   4   0   0   9,491,482,365\n",
      "[ 145]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 146]   4   0   6   0   0   0   9,497,110,863\n",
      "[ 147]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 148]   1   0   5   3   1   0   2,379,532,651\n",
      "[ 149]   2   0   6   2   0   0   4,753,026,805\n",
      "[ 150]   1   0   5   2   2   0   2,379,487,651\n",
      "[ 151]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 152]   1   0   7   1   0   1   2,382,341,901\n",
      "[ 153]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 154]   2   0   4   4   0   0   4,750,212,556\n",
      "[ 155]   1   1   2   4   2   0   2,431,993,430\n",
      "[ 156]   1   0   6   1   2   0   2,380,894,776\n",
      "[ 157]   4   0   3   2   1   0   9,492,844,490\n",
      "[ 158]   1   0   3   4   2   0   2,376,673,402\n",
      "[ 159]   0   0   2   6   2   0       3,224,249\n",
      "[ 160]   0   2   0   5   2   1     113,814,304\n",
      "[ 161]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 162]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 163]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 164]   0   0   6   3   1   0       8,897,747\n",
      "[ 165]   5   0   3   1   1   0  11,864,886,519\n",
      "[ 166]   0   0   5   5   0   0       7,535,622\n",
      "[ 167]   0   0   7   3   0   0      10,349,872\n",
      "[ 168]   1   0   5   3   1   0   2,379,532,651\n",
      "[ 169]   1   0   3   6   0   0   2,376,763,402\n",
      "[ 170]   0   0   6   4   0   0       8,942,747\n",
      "[ 171]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 172]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 173]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 174]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 175]   3   0   6   0   1   0   7,125,023,834\n",
      "[ 176]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 177]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 178]   2   0   3   4   1   0   4,748,760,431\n",
      "[ 179]   3   0   3   4   0   0   7,120,847,461\n",
      "[ 180]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 181]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 182]   0   0   3   7   0   0       4,721,373\n",
      "[ 183]   6   1   2   1   0   0  14,292,293,575\n",
      "[ 184]   1   0   4   4   1   0   2,378,125,527\n",
      "[ 185]   2   1   4   1   2   0   4,806,849,708\n",
      "[ 186]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 187]   2   0   4   4   0   0   4,750,212,556\n",
      "[ 188]   5   1   2   2   0   0  11,920,251,546\n",
      "[ 189]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 190]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 191]   2   0   6   2   0   0   4,753,026,805\n",
      "[ 192]   2   0   4   2   2   0   4,750,122,556\n",
      "[ 193]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 194]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 195]   2   0   3   5   0   0   4,748,805,431\n",
      "[ 196]   1   0   7   2   0   0   2,382,391,901\n",
      "[ 197]   2   1   5   2   0   0   4,808,346,833\n",
      "[ 198]   1   1   5   3   0   0   2,436,304,804\n",
      "[ 199]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 200]   0   1   6   2   1   0      65,624,899\n",
      "[ 201]   1   1   4   2   2   0   2,434,807,679\n",
      "[ 202]   4   0   6   0   0   0   9,497,110,863\n",
      "[ 203]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 204]   2   0   5   2   1   0   4,751,574,681\n",
      "[ 205]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 206]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 207]   0   0   5   4   1   0       7,490,622\n",
      "[ 208]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 209]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 210]   4   0   2   4   0   0   9,491,482,365\n",
      "[ 211]   3   1   4   2   0   0   7,178,981,737\n",
      "[ 212]   3   0   3   4   0   0   7,120,847,461\n",
      "[ 213]   1   1   3   1   4   0   2,433,310,554\n",
      "[ 214]   1   0   2   3   4   0   2,375,176,278\n",
      "[ 215]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 216]   5   1   2   2   0   0  11,920,251,546\n",
      "[ 217]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 218]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 219]   4   0   6   0   0   0   9,497,110,863\n",
      "[ 220]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 221]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 222]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 223]   2   1   5   2   0   0   4,808,346,833\n",
      "[ 224]   2   0   4   3   1   0   4,750,167,556\n",
      "[ 225]   0   0   3   5   2   0       4,631,373\n",
      "[ 226]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 227]   0   0   1   7   1   1       1,812,124\n",
      "[ 228]   2   0   5   1   2   0   4,751,529,681\n",
      "[ 229]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 230]   2   0   5   2   0   1   4,751,569,681\n",
      "[ 231]   2   0   3   5   0   0   4,748,805,431\n",
      "[ 232]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 233]   7   0   3   0   0   0  16,609,015,577\n",
      "[ 234]   6   0   2   2   0   0  14,235,566,423\n",
      "[ 235]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 236]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 237]   5   0   3   1   1   0  11,864,886,519\n",
      "[ 238]   1   0   6   2   1   0   2,380,939,776\n",
      "[ 239]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 240]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 241]   1   2   3   4   0   0   2,490,217,707\n",
      "[ 242]   5   1   4   0   0   0  11,923,065,795\n",
      "[ 243]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 244]   2   0   5   1   2   0   4,751,529,681\n",
      "[ 245]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 246]   2   0   4   3   1   0   4,750,167,556\n",
      "[ 247]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 248]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 249]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 250]   3   1   5   1   0   0   7,180,388,862\n",
      "[ 251]   1   0   6   2   1   0   2,380,939,776\n",
      "[ 252]   4   0   2   3   1   0   9,491,437,365\n",
      "[ 253]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 254]   2   0   8   0   0   0   4,755,841,054\n",
      "[ 255]   0   0   8   2   0   0      11,756,996\n",
      "[ 256]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 257]   2   0   4   2   2   0   4,750,122,556\n",
      "[ 258]   1   0   4   3   2   0   2,378,080,527\n",
      "[ 259]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 260]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 261]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 262]   0   1   7   1   1   0      67,032,024\n",
      "[ 263]   2   0   5   2   1   0   4,751,574,681\n",
      "[ 264]   5   1   4   0   0   0  11,923,065,795\n",
      "[ 265]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 266]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 267]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 268]   2   0   7   0   1   0   4,754,388,930\n",
      "[ 269]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 270]   4   1   5   0   0   0   9,552,430,891\n",
      "[ 271]   2   0   8   0   0   0   4,755,841,054\n",
      "[ 272]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 273]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 274]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 275]   2   0   4   4   0   0   4,750,212,556\n",
      "[ 276]   2   0   2   5   1   0   4,747,353,307\n",
      "[ 277]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 278]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 279]   7   0   2   1   0   0  16,607,608,452\n",
      "[ 280]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 281]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 282]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 283]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 284]   4   1   5   0   0   0   9,552,430,891\n",
      "[ 285]   1   0   8   1   0   0   2,383,799,025\n",
      "[ 286]   6   0   4   0   0   0  14,238,380,672\n",
      "[ 287]   0   0   5   4   1   0       7,490,622\n",
      "[ 288]   2   0   6   0   1   1   4,752,931,805\n",
      "[ 289]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 290]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 291]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 292]   6   0   3   0   1   0  14,236,928,548\n",
      "[ 293]   5   0   2   3   0   0  11,863,524,394\n",
      "[ 294]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 295]   6   0   3   0   1   0  14,236,928,548\n",
      "[ 296]   0   0   5   5   0   0       7,535,622\n",
      "[ 297]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 298]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 299]   3   1   2   4   0   0   7,176,167,488\n",
      "[ 300]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 301]   2   1   5   1   1   0   4,808,301,833\n",
      "[ 302]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 303]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 304]   1   0   4   5   0   0   2,378,170,527\n",
      "[ 305]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 306]   1   0   5   4   0   0   2,379,577,651\n",
      "[ 307]   1   0   8   1   0   0   2,383,799,025\n",
      "[ 308]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 309]   1   0   8   1   0   0   2,383,799,025\n",
      "[ 310]   7   0   1   2   0   0  16,606,201,328\n",
      "[ 311]   1   0   2   6   0   1   2,375,306,278\n",
      "[ 312]   2   0   2   6   0   0   4,747,398,307\n",
      "[ 313]   3   1   5   1   0   0   7,180,388,862\n",
      "[ 314]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 315]   4   1   1   2   2   0   9,546,712,393\n",
      "[ 316]   0   1   7   1   1   0      67,032,024\n",
      "[ 317]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 318]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 319]   6   0   3   1   0   0  14,236,973,548\n",
      "[ 320]   7   0   2   1   0   0  16,607,608,452\n",
      "[ 321]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 322]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 323]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 324]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 325]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 326]   4   0   5   0   1   0   9,495,658,739\n",
      "[ 327]   4   0   6   0   0   0   9,497,110,863\n",
      "[ 328]   3   1   6   0   0   0   7,181,795,986\n",
      "[ 329]   3   0   5   1   1   0   7,123,616,710\n",
      "[ 330]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 331]   2   0   3   5   0   0   4,748,805,431\n",
      "[ 332]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 333]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 334]   2   1   7   0   0   0   4,811,161,082\n",
      "[ 335]   3   1   4   1   1   0   7,178,936,737\n",
      "[ 336]   5   0   2   3   0   0  11,863,524,394\n",
      "[ 337]   3   0   4   1   2   0   7,122,164,585\n",
      "[ 338]   0   0   7   3   0   0      10,349,872\n",
      "[ 339]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 340]   0   0   6   4   0   0       8,942,747\n",
      "[ 341]   1   0   5   1   3   0   2,379,442,651\n",
      "[ 342]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 343]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 344]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 345]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 346]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 347]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 348]   1   1   6   2   0   0   2,437,711,928\n",
      "[ 349]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 350]   0   0   7   3   0   0      10,349,872\n",
      "[ 351]   5   0   4   1   0   0  11,866,338,643\n",
      "[ 352]   0   0   6   4   0   0       8,942,747\n",
      "[ 353]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 354]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 355]   2   0   6   2   0   0   4,753,026,805\n",
      "[ 356]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 357]   1   0   4   4   1   0   2,378,125,527\n",
      "[ 358]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 359]   0   0   7   3   0   0      10,349,872\n",
      "[ 360]   2   0   6   1   1   0   4,752,981,805\n",
      "[ 361]   6   0   2   2   0   0  14,235,566,423\n",
      "[ 362]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 363]   4   0   2   4   0   0   9,491,482,365\n",
      "[ 364]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 365]   6   0   2   0   2   0  14,235,476,423\n",
      "[ 366]   1   0   5   4   0   0   2,379,577,651\n",
      "[ 367]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 368]   4   0   4   0   2   0   9,494,206,614\n",
      "[ 369]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 370]   4   0   4   2   0   0   9,494,296,614\n",
      "[ 371]   3   1   4   2   0   0   7,178,981,737\n",
      "[ 372]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 373]   1   0   6   3   0   0   2,380,984,776\n",
      "[ 374]   3   0   6   0   1   0   7,125,023,834\n",
      "[ 375]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 376]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 377]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 378]   4   0   5   1   0   0   9,495,703,739\n",
      "[ 379]   2   0   7   0   1   0   4,754,388,930\n",
      "[ 380]   6   0   4   0   0   0  14,238,380,672\n",
      "[ 381]   3   2   2   2   1   0   7,232,849,640\n",
      "[ 382]   0   0   8   2   0   0      11,756,996\n",
      "[ 383]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 384]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 385]   2   0   7   1   0   0   4,754,433,930\n",
      "[ 386]   1   0   6   2   1   0   2,380,939,776\n",
      "[ 387]   3   0   6   1   0   0   7,125,068,834\n",
      "[ 388]   2   0   4   3   0   1   4,750,162,556\n",
      "[ 389]   3   0   4   3   0   0   7,122,254,585\n",
      "[ 390]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 391]   2   0   5   3   0   0   4,751,619,681\n",
      "[ 392]   5   0   5   0   0   0  11,867,745,768\n",
      "[ 393]   3   0   5   2   0   0   7,123,661,710\n",
      "[ 394]   4   0   3   3   0   0   9,492,889,490\n",
      "[ 395]   5   0   2   3   0   0  11,863,524,394\n",
      "[ 396]   3   0   7   0   0   0   7,126,475,959\n",
      "[ 397]   2   0   6   1   1   0   4,752,981,805\n",
      "[ 398]   1   1   6   2   0   0   2,437,711,928\n",
      "[ 399]   5   0   3   2   0   0  11,864,931,519\n",
      "[ 400]   2   0   7   0   1   0   4,754,388,930\n",
      "[ 401]   0   0   0   0   0  10               0\n",
      "[ 402]   0   0   0   0   0  10               0\n",
      "[ 403]   0   0   0   0   0  10               0\n",
      "[ 404]   0   0   0   0   0  10               0\n",
      "[ 405]   0   0   0   0   0  10               0\n",
      "[ 406]   0   0   0   0   0  10               0\n",
      "[ 407]   0   0   0   0   0  10               0\n",
      "[ 408]   0   0   0   0   0  10               0\n",
      "[ 409]   0   0   0   0   0  10               0\n",
      "[ 410]   0   0   0   0   0  10               0\n",
      "[ 411]   0   0   0   1   2   7          60,000\n",
      "[ 412]   0   0   0   0   0  10               0\n",
      "[ 413]   0   0   0   0   0  10               0\n",
      "[ 414]   0   0   0   0   0  10               0\n",
      "[ 415]   0   0   0   0   0  10               0\n",
      "[ 416]   0   0   0   0   2   8          10,000\n",
      "[ 417]   0   0   0   0   0  10               0\n",
      "[ 418]   0   0   0   0   0  10               0\n",
      "[ 419]   0   0   0   0   0  10               0\n",
      "[ 420]   0   0   0   0   0  10               0\n",
      "[ 421]   0   0   0   0   0  10               0\n",
      "[ 422]   0   0   0   0   0  10               0\n",
      "[ 423]   0   0   0   0   0  10               0\n",
      "[ 424]   0   0   0   0   0  10               0\n",
      "[ 425]   0   0   0   0   1   9           5,000\n",
      "[ 426]   0   0   0   0   0  10               0\n",
      "[ 427]   0   0   0   0   0  10               0\n",
      "[ 428]   0   0   0   0   0  10               0\n",
      "[ 429]   0   0   0   0   0  10               0\n",
      "[ 430]   0   0   0   0   0  10               0\n",
      "[ 431]   0   0   0   0   0  10               0\n",
      "[ 432]   0   0   0   0   0  10               0\n",
      "[ 433]   0   0   0   0   1   9           5,000\n",
      "[ 434]   0   0   0   0   0  10               0\n",
      "[ 435]   0   0   0   0   0  10               0\n",
      "[ 436]   0   0   0   0   0  10               0\n",
      "[ 437]   0   0   0   0   0  10               0\n",
      "[ 438]   0   0   0   0   0  10               0\n",
      "[ 439]   0   0   0   0   0  10               0\n",
      "[ 440]   0   0   0   0   0  10               0\n",
      "[ 441]   0   0   0   0   1   9           5,000\n",
      "[ 442]   0   0   0   0   0  10               0\n",
      "[ 443]   0   0   0   0   0  10               0\n",
      "[ 444]   0   0   0   0   0  10               0\n",
      "[ 445]   0   0   0   0   0  10               0\n",
      "[ 446]   0   0   0   0   0  10               0\n",
      "[ 447]   0   0   0   0   0  10               0\n",
      "[ 448]   0   0   0   0   0  10               0\n",
      "[ 449]   0   0   0   0   0  10               0\n",
      "[ 450]   0   0   0   0   0  10               0\n",
      "[ 451]   0   0   0   0   0  10               0\n",
      "[ 452]   0   0   0   0   0  10               0\n",
      "[ 453]   0   0   0   0   0  10               0\n",
      "[ 454]   0   0   0   0   4   6          20,000\n",
      "[ 455]   0   0   0   0   0  10               0\n",
      "[ 456]   0   0   0   0   0  10               0\n",
      "[ 457]   0   0   0   0   0  10               0\n",
      "[ 458]   0   0   0   0   0  10               0\n",
      "[ 459]   0   0   0   0   0  10               0\n",
      "[ 460]   0   0   0   0   0  10               0\n",
      "[ 461]   0   0   0   0   0  10               0\n",
      "[ 462]   0   0   0   0   0  10               0\n",
      "[ 463]   0   0   0   0   1   9           5,000\n",
      "[ 464]   0   0   0   0   0  10               0\n",
      "[ 465]   0   0   0   0   0  10               0\n",
      "[ 466]   0   0   0   0   0  10               0\n",
      "[ 467]   0   0   0   0   0  10               0\n",
      "[ 468]   0   0   0   0   0  10               0\n",
      "[ 469]   0   0   0   0   0  10               0\n",
      "[ 470]   0   0   0   0   0  10               0\n",
      "[ 471]   0   0   0   0   0  10               0\n",
      "[ 472]   0   0   0   0   0  10               0\n",
      "[ 473]   0   0   0   0   0  10               0\n",
      "[ 474]   0   0   0   0   1   9           5,000\n",
      "[ 475]   0   0   0   0   0  10               0\n",
      "[ 476]   0   0   0   0   0  10               0\n",
      "[ 477]   0   0   0   0   0  10               0\n",
      "[ 478]   0   0   0   0   0  10               0\n",
      "[ 479]   0   0   0   0   0  10               0\n",
      "[ 480]   0   0   0   0   0  10               0\n",
      "[ 481]   0   0   0   0   0  10               0\n",
      "[ 482]   0   0   0   0   0  10               0\n",
      "[ 483]   0   0   0   0   0  10               0\n",
      "[ 484]   0   0   0   0   0  10               0\n",
      "[ 485]   0   0   0   0   0  10               0\n",
      "[ 486]   0   0   0   0   2   8          10,000\n",
      "[ 487]   0   0   0   0   0  10               0\n",
      "[ 488]   0   0   0   0   0  10               0\n",
      "[ 489]   0   0   0   0   0  10               0\n",
      "[ 490]   0   0   0   0   0  10               0\n",
      "[ 491]   0   0   0   0   0  10               0\n",
      "[ 492]   0   0   0   0   0  10               0\n",
      "[ 493]   0   0   0   0   0  10               0\n",
      "[ 494]   0   0   0   0   0  10               0\n",
      "[ 495]   0   0   0   0   0  10               0\n",
      "[ 496]   0   0   0   0   0  10               0\n",
      "[ 497]   0   0   0   0   0  10               0\n",
      "[ 498]   0   0   0   0   0  10               0\n",
      "[ 499]   0   0   0   0   2   8          10,000\n",
      "[ 500]   0   0   0   0   1   9           5,000\n",
      "[ 501]   0   0   0   0   0  10               0\n",
      "[ 502]   0   0   0   0   0  10               0\n",
      "[ 503]   0   0   0   0   0  10               0\n",
      "[ 504]   0   0   0   0   0  10               0\n",
      "[ 505]   0   0   0   0   0  10               0\n",
      "[ 506]   0   0   0   0   0  10               0\n",
      "[ 507]   0   0   0   0   1   9           5,000\n",
      "[ 508]   0   0   0   0   0  10               0\n",
      "[ 509]   0   0   0   0   0  10               0\n",
      "[ 510]   0   0   0   0   0  10               0\n",
      "[ 511]   0   0   0   0   1   9           5,000\n",
      "[ 512]   0   0   0   0   3   7          15,000\n",
      "[ 513]   0   0   0   0   1   9           5,000\n",
      "[ 514]   0   0   0   0   0  10               0\n",
      "[ 515]   0   0   0   0   0  10               0\n",
      "[ 516]   0   0   0   0   0  10               0\n",
      "[ 517]   0   0   0   0   0  10               0\n",
      "[ 518]   0   0   0   0   0  10               0\n",
      "[ 519]   0   0   0   0   0  10               0\n",
      "[ 520]   0   0   0   0   0  10               0\n",
      "[ 521]   0   0   0   0   0  10               0\n",
      "[ 522]   0   0   0   0   0  10               0\n",
      "[ 523]   0   0   0   0   0  10               0\n",
      "[ 524]   0   0   0   0   0  10               0\n",
      "[ 525]   0   0   0   0   0  10               0\n",
      "[ 526]   0   0   0   0   0  10               0\n",
      "[ 527]   0   0   0   0   0  10               0\n",
      "[ 528]   0   0   0   0   0  10               0\n",
      "[ 529]   0   0   0   0   0  10               0\n",
      "[ 530]   0   0   0   0   0  10               0\n",
      "[ 531]   0   0   0   0   0  10               0\n",
      "[ 532]   0   0   0   0   0  10               0\n",
      "[ 533]   0   0   0   0   0  10               0\n",
      "[ 534]   0   0   0   0   0  10               0\n",
      "[ 535]   0   0   0   0   0  10               0\n",
      "[ 536]   0   0   0   0   0  10               0\n",
      "Total\n",
      "==========\n",
      "Train  1011    54  1850   896   167    22 2,403,992,323,107\n",
      "Val       0     0     0     1    24  1315         140,000\n",
      "Test      0     0     0     0     0     0          30,000\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "train_total_reward = []\n",
    "train_total_grade = np.zeros(6, dtype=int)\n",
    "\n",
    "val_total_reward = []\n",
    "val_total_grade = np.zeros(6, dtype=int)\n",
    "\n",
    "test_total_reward = []\n",
    "test_total_grade = np.zeros(6, dtype=int)\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "print('[No. ] 1st 2nd 3rd 4th 5th 6th Rewards')\n",
    "\n",
    "for i in range(len(x_samples)):\n",
    "    xs = x_samples[i].reshape(1, 1, 45)\n",
    "    ys_pred = model.predict_on_batch(xs) # 모델의 출력값을 얻음\n",
    "    \n",
    "    sum_reward = 0\n",
    "    sum_grade = np.zeros(6, dtype=int) # 6등까지 변수\n",
    "\n",
    "    for n in range(10): # 10판 수행\n",
    "        numbers = gen_numbers_from_probability(ys_pred[0])\n",
    "        \n",
    "        #i회차 입력 후 나온 출력을 i+1회차와 비교함\n",
    "        grade, reward = calc_reward(rows[i+1,1:7], rows[i+1,7], numbers) \n",
    "        \n",
    "        sum_reward += reward\n",
    "        sum_grade[grade] += 1\n",
    "\n",
    "        if i >= train_idx[0] and i < train_idx[1]:\n",
    "            train_total_grade[grade] += 1\n",
    "        elif i >= val_idx[0] and i < val_idx[1]:\n",
    "            val_total_grade[grade] += 1\n",
    "        elif i >= test_idx[0] and i < test_idx[1]:\n",
    "            val_total_grade[grade] += 1\n",
    "    \n",
    "    if i >= train_idx[0] and i < train_idx[1]:\n",
    "        train_total_reward.append(sum_reward)\n",
    "    elif i >= val_idx[0] and i < val_idx[1]:\n",
    "        val_total_reward.append(sum_reward)\n",
    "    elif i >= test_idx[0] and i < test_idx[1]:\n",
    "        test_total_reward.append(sum_reward)\n",
    "                        \n",
    "    print('[{0:4d}] {1:3d} {2:3d} {3:3d} {4:3d} {5:3d} {6:3d} {7:15,d}'.format(i+1, sum_grade[0], sum_grade[1], sum_grade[2], sum_grade[3], sum_grade[4], sum_grade[5], int(sum_reward)))\n",
    "\n",
    "print('Total') \n",
    "print('==========')    \n",
    "print('Train {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(train_total_grade[0], train_total_grade[1], train_total_grade[2], train_total_grade[3], train_total_grade[4], train_total_grade[5], int(sum(train_total_reward))))\n",
    "print('Val   {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(val_total_grade[0], val_total_grade[1], val_total_grade[2], val_total_grade[3], val_total_grade[4], val_total_grade[5], int(sum(val_total_reward))))\n",
    "print('Test  {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(test_total_grade[0], test_total_grade[1], test_total_grade[2], test_total_grade[3], test_total_grade[4], test_total_grade[5], int(sum(test_total_reward))))\n",
    "print('==========')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검증셋 없이 모든 데이터를 다시 훈련시킨다.\n",
    "# 최대 100번 에포크까지 수행\n",
    "for epoch in range(100):\n",
    "\n",
    "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
    "\n",
    "    for i in range(len(x_samples)):\n",
    "        \n",
    "        xs = x_samples[i].reshape(1, 1, 45)\n",
    "        ys = y_samples[i].reshape(1, 45)\n",
    "        \n",
    "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
    "\n",
    "        batch_train_loss.append(loss)\n",
    "        batch_train_acc.append(acc)\n",
    "\n",
    "    train_loss.append(np.mean(batch_train_loss))\n",
    "    train_acc.append(np.mean(batch_train_acc))\n",
    "\n",
    "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receive numbers\n",
      "0 : [1, 6, 16, 23, 37, 39]\n",
      "1 : [6, 13, 29, 37, 39, 43]\n",
      "2 : [6, 16, 23, 29, 37, 39]\n",
      "3 : [5, 6, 16, 23, 37, 39]\n",
      "4 : [6, 11, 16, 23, 37, 39]\n",
      "5 : [13, 16, 23, 37, 42, 43]\n",
      "6 : [6, 12, 21, 23, 29, 37]\n",
      "7 : [1, 6, 16, 29, 37, 39]\n",
      "8 : [6, 16, 23, 29, 37, 39]\n",
      "9 : [6, 19, 23, 29, 37, 44]\n"
     ]
    }
   ],
   "source": [
    "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
    "\n",
    "print('receive numbers')\n",
    "\n",
    "xs = x_samples[-1].reshape(1, 1, 45)\n",
    "\n",
    "ys_pred = model.predict_on_batch(xs)\n",
    "\n",
    "list_numbers = []\n",
    "\n",
    "for n in range(10):\n",
    "    numbers = gen_numbers_from_probability(ys_pred[0])\n",
    "    numbers.sort()\n",
    "    print('{0} : {1}'.format(n, numbers))\n",
    "    list_numbers.append(numbers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
